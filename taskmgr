#!/usr/bin/env python
# -*- coding: UTF-8 -*-
from fabric import main as fabricmain
from fabric.api import *
import argparse
import logging
import configparser
import os.path
import sys
import os
import time
import subprocess
import re
import signal
from random import randint

import multiprocessing
import Queue


__version__ = "0.0.1"

class OperationException(Exception):
    pass

class ParseException(Exception):
    pass

class TaskRunException(Exception):
    pass

class TaskMgr(object):

    def __init__(self):
        # read cli arguments
        self.args = self.parse_cli_args()
        self.config = dict()
        self.lock = None
        self.worker_pools = list()

    def get_arguments(self):
        return self.args

    def read_configuration(self, configfile, ctype, allow_no_value=False):
        config = configparser.ConfigParser(allow_no_value=allow_no_value)
        config._interpolation = configparser.ExtendedInterpolation()
        try:
            config.read(configfile)
        except Exception as e:
            raise OperationException("Failed to read type '%s' config file %s: %s" % (ctype, configfile, str(e)))
        self.config[ctype] = config._sections

    def get_configuration(self, ctype):
        return self.config[ctype]
    
    def parse_cli_args(self):
        # parse command line arguments
        parser = argparse.ArgumentParser()
        parser.add_argument("-v", "--version", action="version", version=__version__)
        parser.add_argument("--cluster-config", help="The location of the cluster configuration file", default="cluster.ini")
        parser.add_argument("--command-config", help="The location of the cluster configuration file", default="commands.ini")
        parser.add_argument("-H", "--hosts-file", help="File storing the hosts to work on, one by line. If not used, the hostnames from cluster.ini are used.") # fixme
        #parser.add_argument("-H", "--hosts", help="Hostnames to work on") # fixme
        parser.add_argument("-n", "--cluster-name", help="The name of the cluster to work on", action='append', required=True)
        parser.add_argument("-c", "--command", help="THe name of the command to run", required=True)
        parser.add_argument("-d", "--debug", help="Show debug information", action="store_true", default=False)
        args = parser.parse_args()
        return args

    #def worker(self, host, cluster):
    #    # create a lock object if it doesn't exist
    #    #if not self.lock:
    #    #    self.lock = Lock()
    #    print "in here"
    #    for command in self.config["command"][self.args.command].keys():
    #        (action, cmd_name) = command.split(" ")
    #        # pair the check name with the one that is defined in the cluster configuration
    #        if not cmd_name in self.config['cluster'][cluster]:
    #            self.handle_exception("Referenced command '%s' is not defined in in '%s' for cluster '%s'!" % (cmd_name, self.args.cluster_config, cluster))
    #        # call the command
    #        if action == "check":
    #            # run the check, fail if result is false
    #            check_name = config["cluster"][cluster][cmd_name].split(":")[0]
    #            check_arguments = ":".join(config["cluster"][cluster][cmd_name].split(":")[1:])
    #            with quiet():
    #                result = execute(check_name, check_arguments, hosts=[host])
    #            #with self.lock:          
    #            if result[host]:
    #                print "%s: all ok" % cluster
    #            else:
    #                print "%s: failed" % cluster
    #        elif action == "wait":
    #            # stop and wait until check returns true
    #            pass
    #        else:
    #            self.handle_exception("Unknown command '%s' defined in '%s'!" % (cmd_name, args.command_config))
    #    time.sleep(randint(2,9))

    #def worker_cb(self, host, cluster):
    #    print "finished with %s on cluster %s" % (host, cluster)

    def append_worker_pool(self, pool):
        self.worker_pools.append(pool)

    def get_worker_pools(self):
        return self.worker_pools


def initialize_logger(logfile, debug, format_file, format_console):
    """ initialize logging - console """
    if debug:
        loglevel = logging.DEBUG
    else:
        loglevel = logging.INFO

    l = logging.getLogger()
    l.setLevel(loglevel)

    fh = logging.FileHandler(logfile)
    fh.setLevel(loglevel)

    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(loglevel)  
    formatter_file = logging.Formatter(format_file)
    formatter_console = logging.Formatter(format_console)
    ch.setFormatter(formatter_console)
    fh.setFormatter(formatter_file)

    l.addHandler(ch)
    l.addHandler(fh)

    return l

def log_info(msg, *args, **kwargs):
    global l
    args, kwargs = extend_logger_information(*args, **kwargs)
    l.info(msg, *args, **kwargs)

def log_debug(msg, *args, **kwargs):
    global l
    args, kwargs = extend_logger_information(*args, **kwargs)
    l.debug(msg, *args, **kwargs)

def log_warning(msg, *args, **kwargs):
    global l
    args, kwargs = extend_logger_information(*args, **kwargs)
    l.warning(msg, *args, **kwargs)

def log_critical(msg, *args, **kwargs):
    global l
    args, kwargs = extend_logger_information(*args, **kwargs)
    l.critical(msg, *args, **kwargs)

def log_exception(msg, *args, **kwargs):
    global l
    args, kwargs = extend_logger_information(*args, **kwargs)
    l.exception(msg, *args, **kwargs)


def extend_logger_information(*args, **kwargs):
    if not "extra" in kwargs:
        kwargs["extra"] = dict()

    # convert args from tuple to list so we can modify it
    newargs = list(args)

    # initialize host and cluster with default values
    host = "local"
    cluster = "none"

    # get host and cluster from args, if provided
    try:
        host, cluster = newargs
        # remove them from args
        newargs.remove(host)
        newargs.remove(cluster)
    except:
        pass

    # add host and cluster to kwargs, will be passed to the logger
    kwargs["extra"]["host"] = host
    kwargs["extra"]["cluster"] = cluster

    # return the modified arguments
    return newargs, kwargs

def handle_exception(exception, fatal=True):
    global l
    if type(exception) == OperationException:
        log_critical(e)
        exit_if_fatal(fatal)

    elif type(exception) == str:
        log_critical(exception)
        exit_if_fatal(fatal)

    elif type(exception) == unicode:
        log_critical(exception)
        exit_if_fatal(fatal)

    else:
        log_critical("Unknown exception type: %s" % type(exception))
        exit_if_fatal(fatal)

def exit_if_fatal(fatal=True):
    if fatal:
        sys.exit(1)


def worker(host, cluster, config, args):
    global l
    global timeout
    global command_timeout

    # set timeout values for facter
    env.timeout = timeout
    env.command_timeout = command_timeout

    # ignore ctrl c here
    signal.signal(signal.SIGINT, signal.SIG_IGN)
    # create a lock object if it doesn't exist
    #print "starting inside on %s" % host

    # command1: segment1 segment2 segment3
    # command2: segment1 segment2 segment3
    for command in config["command"][args.command].keys():
        #print "command is", command
        #print "running check %s for cluster %s" % (check_name, cluster)
        (action, cmd_name) = command.split(" ")
        # pair the check name with the one that is defined in the cluster configuration
        if not cmd_name in config['cluster'][cluster]:
            handle_exception("Referenced command '%s' is not defined in in '%s' for cluster '%s'!" % (cmd_name, args.cluster_config, cluster))
        # run the check, fail if result is false
        # remove all|any form the end

        # split the command on spaces
        cmd_segments = config["cluster"][cluster][cmd_name].split(" ")
        # check if all|any is at the end, if yes, remove it

        if if cmd_segments[-1].lower() == "any" or if cmd_segments[-1].lower() == "all":
            if cmd_segments[-1].lower() == "any":
                multiple = False
            elif cmd_segments[-1].lower() == "all":
                multiple = True
            # remove it
            cmd_segments.pop(-1)
        else:
            multiple = False

        # call the commands
        # iterate on the segments, process them one by one
        for cmd in cmd_segments:
            check_name = cmd.split(":")[0]
            check_arguments = ":".join(cmd.split(":")[1:])
            if action == "check":
                with quiet():
                    try: 
                        result = execute(check_name, check_arguments, hosts=[host])
                    except Exception as e:
                        handle_exception("Failed to run fabric task '%s' on host '%s' (cluster '%s'): %s" % (check_name, host, cluster, str(e)))

                # got the result, good
                # if multiple is True, 
                if multiple:
                    if result[host]:
                        continue
                    else:
                        return False
                else:
                    if result[host]:
                        return True
                    else:
                        continue
            elif action == "wait":
                # stop and wait until check returns true
                #check_name = config["cluster"][cluster][cmd_name].split(":")[0]
                #check_arguments = ":".join(config["cluster"][cluster][cmd_name].split(":")[1:])
                with quiet():
                    try:
                        log_info("Waiting until check '%s' returns..." % check_name, host, cluster)
                        while True:
                            # multiple: true 
                            #   need to wait for all of them to return OK
                            # multiple: false
                            #   one is enough                           
                            result = execute(check_name, check_arguments, hosts=[host])
                            if result[host] == True:
                                if multiple:
                                    return True
                                else:
                                    break
                            else:
                                log_debug("Waiting before retrying", host, cluster)
                                time.sleep(5)
                                log_debug("Trying again", host, cluster)
                        log_info("Got response, moving on", host, cluster)
                    except Exception as e:
                        handle_exception("Failed to run fabric task '%s' on host '%s' (cluster '%s'): %s" % (check_name, host, cluster, str(e)))

        else:
            handle_exception("Unknown command '%s' defined in '%s'!" % (cmd_name, args.command_config))    

    # fixme: remove this!
    #time.sleep(randint(2,9))
    return (host, cluster)

# due to a possible bug in the signal handling of multiprocessing's Pool implementation
# we need our own pool management
# http://bryceboe.com/2010/08/26/python-multiprocessing-and-keyboardinterrupt/
class AsyncWorker:
    def __init__(self):
        self.job_queue = multiprocessing.Queue()
        self.result_queue = multiprocessing.Queue() 
        self.workers = list()

    def add_job(self, host, cluster, config, args):
        self.job_queue.put((host, cluster, config, args))

    def start_workers(self, worker_func, max_parallel):
        for i in range(max_parallel):
            w = multiprocessing.Process(target=self._worker_helper, args=(worker_func,))
            w.start()
            self.workers.append(w)

    def _worker_helper(self, worker_func):
        while not self.job_queue.empty():
            try:
                host, cluster, config, args = self.job_queue.get(block=False)
                self.result_queue.put(worker_func(host, cluster, config, args)) # fixme, change to kwargs
            except Queue.Empty:
                pass
            except KeyboardInterrupt:
                pass
            except Exception:
                raise

    def wait(self):
        for worker in self.workers:
            worker.join()

    def finish(self):
        for worker in self.workers:
            worker.terminate()
            worker.join()

    def result_queue_empty(self):
        return self.result_queue.empty()

    def result_queue_get(self, block=False):
        return self.result_queue.get(block)

    def close_queues(self):
        while not self.job_queue.empty():
            self.job_queue.get()
        self.job_queue.close()
###################################################################################3

# global variable holding logger object
l = object
# timeouts to use
timeout = 10
command_timeout = None

# load available fabric task files 
# http://stackoverflow.com/questions/23605418/in-fabric-how-can-i-execute-tasks-from-another-python-file
docstring, callables, default = fabricmain.load_fabfile('fabfile')
fabricmain.state.commands.update(callables)

if __name__ == '__main__':

    config = dict()

    # create taskmanager object
    taskmgr = TaskMgr()
    args = taskmgr.get_arguments()

    # initialize logger
    try:
        format_file = '%(asctime)s %(host)s/%(cluster)s %(levelname)s - %(message)s'
        format_console = '%(host)s/%(cluster)s %(levelname)s - %(message)s'
        l = initialize_logger('taskmgr.log', args.debug, format_file, format_console)
    except Exception as e:
        print "Can't initialize logger: ", str(e)
        sys.exit(1)

    # parse the cluster configuration
    try:
        taskmgr.read_configuration(args.cluster_config, ctype="cluster")
    except OperationException as e:
        handle_exception(e)
    # get the parsed configuration in the form of a dict
    config["cluster"] = taskmgr.get_configuration("cluster")

    # parse the command configuration
    try:
        taskmgr.read_configuration(args.command_config, ctype="command", allow_no_value=True)
    except OperationException as e:
        handle_exception(e)
    # get the parsed configuration in the form of a dict
    config["command"] = taskmgr.get_configuration("command")

    if not args.command in config["command"].keys():
        handle_exception("Configuration in '%s' for command '%s' doesn't exist!" % (args.command_config, args.command))

    # iterate on the provided cluster names
    
    for cluster in args.cluster_name:

        # check if the cluster definition exists
        if not cluster in config["cluster"].keys():
            handle_exception("Configuration in '%s' for cluster '%s' doesn't exist!" % (args.cluster_config, cluster))

        # check if a hostlist was provided
        if not args.hosts_file:
            # not provided, use the hostnames defined in cluster.ini
            hosts = subprocess.Popen(["/bin/bash", "-c", "echo %s" % config["cluster"][cluster]['hosts']], stdout=subprocess.PIPE).communicate()[0].rstrip().split(" ")
        else:
            # get the hosts from the host file which are matching the pattern given for the current cluster in cluster.ini
            # expand the expression found in cluster.ini using bash - might need to rewrite this in pure python?
            hosts = list()
            try:
                f = open(args.hosts_file)
                for host in f.readlines():
                    if re.search("(^\s*$|^#)", host):
                        continue
                    if re.search(config["cluster"][cluster]["pattern"], host, re.IGNORECASE):
                        hosts.append(host.rstrip())
            except Exception as e: 
                handle_exception("Problem while reading host file '%s': %s" % (args.hosts_file, str(e)))

        async_worker = AsyncWorker()
        taskmgr.append_worker_pool(async_worker)

        # launch workers
        for host in hosts:
            async_worker.add_job(host, cluster, config, args)
        
        # launch the workers
        async_worker.start_workers(worker, int(config["cluster"][cluster]["max_parallel"]))

    try:
        for async_worker in taskmgr.get_worker_pools():
            async_worker.wait()
            # close all queues
            async_worker.close_queues()            
    except KeyboardInterrupt:
        print "Received Ctrl + C, exiting..."
        # don't wait, just close queues
        for async_worker in taskmgr.get_worker_pools():
            # finish processing jobs, close queues
            async_worker.finish()
            async_worker.close_queues()
        sys.exit(1)
    #finally:
    #    for async_worker in taskmgr.get_worker_pools():
    #        while not async_worker.result_queue_empty():
    #            print async_worker.result_queue_get(block=False)
            

    print "OK"
    sys.exit(0)

    ## execute the steps in the command definition file
    #for command in config["command"][args.command].keys():
    #    (action, cmd_name) = command.split(" ")
    #    # pair the check name with the one that is defined in the cluster configuration
    #    if not cmd_name in config['cluster'][args.cluster_name]:
    #        taskmgr.handle_exception("Referenced command '%s' is not defined in in '%s' for cluster '%s'!" % (cmd_name, args.cluster_config, args.cluster_name))
    #    # call the command
    #    if action == "check":
    #        # run the check, fail if result is false
    #        hosts = [ "3capp-webde-dev01", "3capp-webde-dev02" ]
    #        check_name = config["cluster"][args.cluster_name][cmd_name].split(":")[0]
    #        check_arguments = ":".join(config["cluster"][args.cluster_name][cmd_name].split(":")[1:])
    #        result = execute(check_name, check_arguments, hosts=hosts)
    #        print result
    #        if result[env.host_string]: 
    #            print "all ok"
    #        else:
    #            print "Failed"
    #    elif action == "wait":
    #        # stop and wait until check returns true
    #        pass
    #    else:
    #        handle_exception("Unknown command '%s' defined in '%s'!" % (cmd_name, args.command_config))

    #try:
    #    print cluster_config[args.cluster_name]["check_host_alive"]
    #    hosts = [ "3capp-webde-dev01" ]
    #    result = execute(cluster_config[args.cluster_name]["check_host_alive"], "22", hosts=hosts)
    #except KeyError:
    #    handle_exception("Desired cluster '%s' doesn't exist in the cluster configuration file '%s'!" % (args.cluster_name, args.cluster_config))
    #except Exception as e:
    #    raise
    #    #handle_exception(e)

