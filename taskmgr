#!/usr/bin/env python
# -*- coding: UTF-8 -*-

# needed to properly import tasks, and to access internal fabric functionality
from fabric import main as fabricmain 
from fabric import network as fabricnetwork
from fabric.api import *

import utils.log
import argparse
import logging
import configparser
import os.path
import sys
import os
import time
import subprocess
import re
import signal

import multiprocessing
import Queue


__version__ = "0.0.2"

class OperationException(Exception):
    pass

class ParseException(Exception):
    pass

class TaskRunException(Exception):
    pass

class NoSuchTaskException(Exception):
    pass

class StopException(Exception):
    pass

class TaskMgr(object):

    def __init__(self):
        # read cli arguments
        self.args = self.parse_cli_args()
        self.config = dict()
        self.lock = None
        self.worker_pools = list()

    def get_arguments(self):
        return self.args

    def read_configuration(self, configfile, ctype, allow_no_value=False):
        config = configparser.ConfigParser(allow_no_value=allow_no_value)
        config._interpolation = configparser.ExtendedInterpolation()
        try:
            config.read(configfile)
        except Exception as e:
            raise OperationException("Failed to read type '%s' config file %s: %s" % (ctype, configfile, str(e)))
        self.config[ctype] = config._sections

    def get_configuration(self, ctype):
        return self.config[ctype]
    
    def parse_cli_args(self):
        # parse command line arguments
        parser = argparse.ArgumentParser()
        parser.add_argument("-v", "--version", action="version", version=__version__)
        parser.add_argument("--cluster-config", help="The location of the cluster configuration file", default="cluster.ini")
        parser.add_argument("--command-config", help="The location of the cluster configuration file", default="commands.ini")
        parser.add_argument("-H", "--hosts-file", help="File storing the hosts to work on, one by line. If not used, the hostnames from cluster.ini are used.") # fixme
        parser.add_argument("-n", "--cluster-name", help="The name of the cluster to work on", action='append', required=True)
        parser.add_argument("-c", "--command", help="THe name of the command to run", required=True)
        parser.add_argument("-d", "--debug", help="Show debug information", action="store_true", default=False)
        parser.add_argument("-x", "--unsafe", help="Enable unsafe operation. (disable host key checking, etc)", action='store_true', default=False)
        args = parser.parse_args()
        return args

    def append_worker_pool(self, pool):
        self.worker_pools.append(pool)

    def get_worker_pools(self):
        return self.worker_pools

    def get_node_list_for_cluster(self, cluster):

        # check if the cluster definition exists
        if not cluster in self.config["cluster"].keys():
            handle_exception("Configuration in '%s' for cluster '%s' doesn't exist!" % (self.args.cluster_config, cluster))

        # check if a hostlist was provided
        if not self.args.hosts_file:
            # not provided, use the hostnames defined in cluster.ini
            hosts = subprocess.Popen(["/bin/bash", "-c", "echo %s" % self.config["cluster"][cluster]['hosts']], stdout=subprocess.PIPE).communicate()[0].rstrip().split(" ")
        else:
            # get the hosts from the host file which are matching the pattern given for the current cluster in cluster.ini
            # expand the expression found in cluster.ini using bash - might need to rewrite this in pure python?
            hosts = list()
            try:
                f = open(self.args.hosts_file)
                for host in f.readlines():
                    if re.search("(^\s*$|^#)", host):
                        continue
                    if re.search(self.config["cluster"][cluster]["pattern"], host, flags=re.IGNORECASE):
                        hosts.append(host.rstrip())
            except Exception as e: 
                handle_exception("Problem while reading host file '%s': %s" % (self.args.hosts_file, str(e)))
        return hosts

    def get_full_node_list(self):
        hosts = list()
        for cluster in sorted(self.args.cluster_name):
            hosts.append(self.get_node_list_for_cluster(cluster))
        # return a flattened list (instead of lists of lists)
        return sorted([item for sublist in hosts for item in sublist])


def handle_exception(exception, *args, **kwargs):
    global pid
    if "fatal" in kwargs.keys():
        fatal = kwargs["fatal"]
    else:
        fatal = True
    
    l.critical(exception, *args)
    
    if os.getpid() == pid:
        # this is the parent process, do cleanup
        if fatal:
            exit_handler(1)
    else:
        # child process
        exit_if_fatal(fatal)

def exit_if_fatal(fatal=True):
    if fatal:
        # close fabric ssh connections
        fabricnetwork.disconnect_all()
        sys.exit(1)



def exit_handler(exit_code=None):
    global taskmgr
    for async_worker in taskmgr.get_worker_pools():
        # finish processing jobs, close queues
        async_worker.finish()
        async_worker.close_queues()

    # close fabric ssh connections
    fabricnetwork.disconnect_all()

    if exit_code is None or exit_code == 0:
        results_flat, results_ok, results_failed = print_statistics()
        
        if exit_code is None:
            if len(results_failed):
                exit_code = 1
            else:
                exit_code = 0
                l.info("")
                l.info("All was OK!")
    sys.exit(exit_code)

def print_statistics():
    global taskmgr
    results = dict()
    actual_list = list()

    for async_worker in taskmgr.get_worker_pools():
        while not async_worker.result_queue_empty():
            host, cluster, result = async_worker.result_queue_get(block=True)
            if not cluster in results:
                results[cluster] = dict()
            results[cluster][host] = result
            actual_list.append(host)

    results_flat = [host for cluster in results.itervalues() for host in cluster.itervalues()]
    results_ok = [i for i in results_flat if i == True]
    results_failed = [i for i in results_flat if i == False]

    # fixme: print the list of nodes that were not executed, using a diff of the two lists
    expected_list = taskmgr.get_full_node_list()
    # get the difference of the expected and the actual lists
    # danger: this will kill ordering (ok), and remove unique elements (should be ok, one host should only be part of one cluster)
    skipped_list = sorted(list(set(expected_list) - set(actual_list)))

    if len(results_flat) > 0:
        l.info("=" * 80)
        l.info("Statistics: ")
        l.info("")
        l.info("Expected: %d" % len(expected_list))
        l.info('Total: %d' % len(results_flat))
        l.info('OK: %d/%0.2f%% |  Failed: %d/%0.2f%%' % (len(results_ok), float(len(results_ok))/float(len(results_flat))*100, len(results_failed), float(len(results_failed))/float(len(results_flat))*100))
        if len(results_failed):
            l.info("")
            l.info("-" * 60)
            l.info("Failed hosts:")
            for cluster in sorted(results.keys()):
                for host in sorted(results[cluster].keys()):
                    if results[cluster][host] == False:
                        l.info("%s / %s" % (cluster, host))
        if len(skipped_list):
            l.info("")
            l.info("-" * 60)
            l.info("Skipped hosts:")
            for host in skipped_list:
                l.info(host)

    return (results_flat, results_ok, results_failed)

def worker(host, cluster, config, args):
    """ The worker function. This is the code that gets executed for each cluster/node combination.
        The final result will represent the success or the failure of the given command on the given machine
        In case of a problem an exception is raised.
    """
    global timeout
    global command_timeout

    # set timeout values for facter
    env.timeout = timeout
    env.command_timeout = command_timeout
    env.abort_on_prompts = True
    env.use_ssh_config = True
    if args.unsafe:
        env.reject_unknown_hosts = False
        env.skip_bad_hosts = False
    else:
        env.reject_unknown_hosts = True
        env.skip_bad_hosts = True

    # ignore ctrl c here
    signal.signal(signal.SIGINT, signal.SIG_IGN)

    # make paramiko shut up
    logging.getLogger("paramiko").setLevel(logging.CRITICAL)

    # create a lock object if it doesn't exist
    #print "starting inside on %s" % host

    # command1: segment1 segment2 segment3
    # command2: segment1 segment2 segment3
    for command in config["command"][args.command].keys():
        #print "command is", command
        #print "running check %s for cluster %s" % (check_name, cluster)
        
        try: 
            (action, cmd_name, erroraction) = command.split(" ")
        except ValueError:
            # erroraction was not provided
            (action, cmd_name) = command.split(" ")
            erroraction = "stop"
        erroractions_available = ["stop", "skip"]
        if not erroraction in erroractions_available:
            handle_exception("Error action '%s' for command '%s/%s %s' is unknown. Please use either of: %s" % (erroraction, args.command, action, cmd_name, ",".join(erroractions_available)))
        
        # pair the check name with the one that is defined in the cluster configuration
        if not cmd_name in config['cluster'][cluster]:
            handle_exception("Referenced command '%s' is not defined in in '%s' for cluster '%s'!" % (cmd_name, args.cluster_config, cluster), host, cluster)
        # run the check, fail if result is false
        # remove all|any form the end

        # split the command on 'cmd:'
        # check if ' (all|any)$' matches the command
        tmp_command = config["cluster"][cluster][cmd_name]
        m = re.search("^(.*) (any|all)$", tmp_command, flags=re.IGNORECASE)
        if m:
            # found, set multiple, and remove it from the end of the string
            tmp_command = m.group(1)
            if m.group(2) == "all":
                multiple = True
            elif m.group(2) == "any":
                multiple = False
            else:
                handle_exception("Syntax error for command '%s' in '%s' for cluster '%s': multiple must be either 'all' or 'any'" % (cmd_name, args.cluster_config, cluster), host, cluster)
        else:
            # default is 'all'
            multiple = True 
        # split commands on 'cmd:', and filter out any empty elements
        cmd_segments = filter(bool, re.split("cmd:", tmp_command, flags=re.IGNORECASE))

        # call the commands
        # rule of thumb: on any failed check we return with false, meaning that the command failed
        # iterate on the segments, process them one by one
        if action == "check":
            try:
                result = run_check_tasks(host, cluster, cmd_name, cmd_segments, multiple)
            except Exception as e:
                raise
            if not result:
                l.error("%s '%s' failed" % (action, cmd_name), host, cluster)
                if erroraction == "skip":
                    l.warning("Error action for %s '%s' is 'skip', moving on with next host, but not stopping" % (action, cmd_name), host, cluster)
                    return (host, cluster, False)
                elif erroraction == "stop":
                    raise StopException("Requested action for %s %s on %s/%s is '%s'" %(action, cmd_name, cluster, host, erroraction))
            else:
                l.info("%s '%s' OK" % (action, cmd_name), host, cluster)
        
        elif action == "wait":
            # stop and wait until check returns true
            try:
                result = run_wait_tasks(host, cluster, cmd_name, cmd_segments, multiple)
            except Exception as e:
                raise
            if not result:
                # for this we would need to implement timeouts in run_wait_tasks, because right now False is never returned
                # in this case will need to evaluate 'erroraction' 
                l.error("%s '%s' failed" % (action, cmd_name), host, cluster)
                return (host, cluster, False)
            else:
                l.info("%s '%s' OK" % (action, cmd_name), host, cluster)

        else:
            handle_exception("Unknown command '%s' defined in '%s'!" % (cmd_name, args.command_config), host, cluster)    

    l.important("All commands for %s/%s have been successfully run" % (cluster, host), host, cluster)
    return (host, cluster, True)

def run_check_tasks(host, cluster, cmd_name, cmd_segments, multiple):
    """ 
        A helper task called from worker() to execute a check task
        This is needed so that multiple checks can be evaluated with a l.critical
        AND condition
    """
    global timeout
    global command_timeout
    global callables
    results = list()
    for cmd in cmd_segments:
        check_name = cmd.split(":")[0]
        check_arguments = ":".join(cmd.split(":")[1:])
        if not check_name in fabricmain._task_names(callables):
            raise NoSuchTaskException("Fabric task '%s' doesn't exist!" % check_name)

        try: 
            l.debug("Executing check %s: '%s %s'" % (cmd_name, check_name, check_arguments), host, cluster)
            with quiet():
                result = execute(check_name, check_arguments, hosts=[host])
        except Exception as e:
            raise TaskRunException("Failed to run fabric task '%s' on host '%s' (cluster '%s'): %s" % (check_name, host, cluster, str(e)))

        # got the result, good
        if multiple:
            if result[host]:
                results.append(result[host])
                continue
            else:
                return False
        else:
            if result[host]:
                return True
            else:
                continue
                results.append(result[host])
    # if we are here all of the checks have been run
    # either because multiple is True, or because multiple is False, but
    # the first checks failed
    if multiple:
        return all(res == True for res in results)
    else:
        return any(res == True for res in results)

def run_wait_tasks(host, cluster, cmd_name, cmd_segments, multiple):
    global timeout
    global command_timeout
    global callables
    results = list()
    for cmd in cmd_segments:
        check_name = cmd.split(":")[0]
        check_arguments = ":".join(cmd.split(":")[1:])
        # check if a given fabric task exists
        if not check_name in fabricmain._task_names(callables):
            raise NoSuchTaskException("Fabric task '%s' doesn't exist!" % check_name)

        l.debug("Starting wait %s: '%s %s' returns..." % (cmd_name, check_name, check_arguments), host, cluster)
        while True:
            # this loop is needed to ensure we wait until the check is finally ok
            try:
                l.debug("Executing wait %s: '%s %s'" % (cmd_name, check_name, check_arguments), host, cluster)
                with quiet():             
                    result = execute(check_name, check_arguments, hosts=[host])
            except Exception as e:
                raise TaskRunException("Failed to run fabric task '%s' on host '%s' (cluster '%s'): %s" % (check_name, host, cluster, str(e)))
            if result[host] == True:
                # break out of while True
                l.debug("Got response, moving on", host, cluster)
                break
            else:
                # failed, have to wait longer
                l.debug("Waiting before retrying", host, cluster)
                time.sleep(5)
                continue
        # we are here, so there was a successful result
        if multiple == False:
            # we can return at this point
            return True
        else:
            results.append(True)
        if not len(results) < len(cmd_segments):
            break
    # if we are here:
    #   * multiple is true
    #   * all waits have finished with success, no need to evaluate results
    return True

# due to a possible bug in the signal handling of multiprocessing's Pool implementation
# we need our own pool management
# http://bryceboe.com/2010/08/26/python-multiprocessing-and-keyboardinterrupt/
class AsyncWorker:
    def __init__(self):
        self.job_queue = multiprocessing.Queue()
        self.result_queue = multiprocessing.Queue() 
        self.workers = list()
        self.failed_clusters = dict()
        #self.failed_nodes = dict()
        #self.lock = multiprocessing.Lock()

    def add_job(self, host, cluster, config, args):
        self.job_queue.put((host, cluster, config, args))

    def start_workers(self, worker_func, max_parallel):
        for i in range(max_parallel):
            w = multiprocessing.Process(target=self._worker_helper, args=(worker_func,))
            w.daemon = True
            w.start()
            self.workers.append(w)

    def _worker_helper(self, worker_func):
        while not self.job_queue.empty():
            try:
                host, cluster, config, args = self.job_queue.get(block=False)

                # implement failing policy:
                # we don't allow any more jobs for faulty nodes
                # but we allow other nodes to finish
                # we don't start any new nodes
                # flags are set in the exception handling part
                if cluster in self.failed_clusters.keys():
                    if self.failed_clusters[cluster]['!alerted!'] == False:
                        l.error("Due to previous errors, new nodes on cluster '%s' are not processed. Already running tasks are allowed to finish." % cluster, host, cluster)
                        self.failed_clusters[cluster]['!alerted!'] = True
                    if host in self.failed_clusters[cluster].keys():
                        if self.failed_clusters[cluster][host] == False:
                            # first occurance, notify
                            l.error("Due to previous errors, no more tasks on '%s/%s' are executed as of now." % (cluster, host), host, cluster)
                            self.failed_clusters[cluster][host] = True
                            continue
                        else:
                            l.warning("skipping instead of starting")
                            continue
                    else:
                        # in the marked cluster, but not marked host, don't allow to continue
                        continue
                else:
                    # not marked cluster, allow to continue
                    pass

                # if here, start the task
                self.result_queue.put(worker_func(host, cluster, config, args)) # put the results to the queue

            except Queue.Empty:
                pass
            except KeyboardInterrupt:
                pass
            except Exception as e:

                l.exception(e, host, cluster)

                # this indicates that a vital task has failed (erroraction==stop)
                # or in case there was only one check at a command, then that one check failed
                # in this case put a result entry in the queue
                self.result_queue.put((host, cluster, False))
                
                # add the given cluster/node to the faulty clusters list
                # mark the cluster as faulty by creating the cluster key if it doesn't already exist
                if not cluster in self.failed_clusters.keys():
                    self.failed_clusters[cluster] = dict()
                    self.failed_clusters[cluster]["!alerted!"] = False

                # mark the node as faulty by adding it to the dict
                # False means that it has not been notified yet
                # after notification flag will be turned true to avoid re-notification
                if not host in self.failed_clusters[cluster].keys():
                    self.failed_clusters[cluster][host] = False

    def wait(self):
        for worker in self.workers:
            worker.join()

    def finish(self):
        for worker in self.workers:
            worker.terminate()
            worker.join()

    def result_queue_empty(self):
        return self.result_queue.empty()

    def result_queue_get(self, block=False):
        return self.result_queue.get(block)

    def close_queues(self):
        while not self.job_queue.empty():
            self.job_queue.get()
        self.job_queue.close()


###################################################################################3

# global variable holding logger object
#l = object
taskmgr = object
# timeouts to use
timeout = 10
command_timeout = None
pid = os.getpid()

# load available fabric task files 
# http://stackoverflow.com/questions/23605418/in-fabric-how-can-i-execute-tasks-from-another-python-file
docstring, callables, default = fabricmain.load_fabfile('fabfile')
fabricmain.state.commands.update(callables)

# initialize logger
try:
    format_file = '%(asctime)s %(clusterinfo)s - %(levelname)s - %(message)s'
    format_console = '$COLOR%(clusterinfo)s%(levelname)s - %(message)s'
    l = utils.log.initialize_logger('taskmgr.log', format_file, format_console)
except Exception as e:
    print "Can't initialize logger: ", str(e)
    sys.exit(1)

if __name__ == '__main__':

    config = dict()

    # create taskmanager object
    taskmgr = TaskMgr()
    args = taskmgr.get_arguments()

    # set the loglevel based on args.debug
    if args.debug:
        loglevel = logging.DEBUG
    else:
        loglevel = logging.INFO
    #l.setLevel(loglevel)
    logging.getLogger().setLevel(loglevel)
    # parse the cluster configuration
    try:
        taskmgr.read_configuration(args.cluster_config, ctype="cluster")
    except OperationException as e:
        handle_exception(e)
    # get the parsed configuration in the form of a dict
    config["cluster"] = taskmgr.get_configuration("cluster")

    # parse the command configuration
    try:
        taskmgr.read_configuration(args.command_config, ctype="command", allow_no_value=True)
    except OperationException as e:
        handle_exception(e)
    # get the parsed configuration in the form of a dict
    config["command"] = taskmgr.get_configuration("command")

    if not args.command in config["command"].keys():
        handle_exception("Configuration in '%s' for command '%s' doesn't exist!" % (args.command_config, args.command))

    # count how many machines will be processed - for statistics later on
    nodelist = taskmgr.get_full_node_list()

    # iterate on the provided cluster names
    for cluster in args.cluster_name:

        hosts = taskmgr.get_node_list_for_cluster(cluster)
        ## check if the cluster definition exists
        #if not cluster in config["cluster"].keys():
        #    handle_exception("Configuration in '%s' for cluster '%s' doesn't exist!" % (args.cluster_config, cluster))
        #
        ## check if a hostlist was provided
        #if not args.hosts_file:
        #    # not provided, use the hostnames defined in cluster.ini
        #    hosts = subprocess.Popen(["/bin/bash", "-c", "echo %s" % config["cluster"][cluster]['hosts']], stdout=subprocess.PIPE).communicate()[0].rstrip().split(" ")
        #else:
        #    # get the hosts from the host file which are matching the pattern given for the current cluster in cluster.ini
        #    # expand the expression found in cluster.ini using bash - might need to rewrite this in pure python?
        #    hosts = list()
        #    try:
        #        f = open(args.hosts_file)
        #        for host in f.readlines():
        #            if re.search("(^\s*$|^#)", host):
        #                continue
        #            if re.search(config["cluster"][cluster]["pattern"], host, re.IGNORECASE):
        #                hosts.append(host.rstrip())
        #    except Exception as e: 
        #        handle_exception("Problem while reading host file '%s': %s" % (args.hosts_file, str(e)))

        async_worker = AsyncWorker()
        taskmgr.append_worker_pool(async_worker)

        # launch workers
        for host in hosts:
            async_worker.add_job(host, cluster, config, args)
        
        # launch the workers
        async_worker.start_workers(worker, int(config["cluster"][cluster]["max_parallel"]))

    try:
        for async_worker in taskmgr.get_worker_pools():
            async_worker.wait()
            # close all queues
            async_worker.close_queues()            
    except KeyboardInterrupt:
        print "Received Ctrl + C, exiting..."
        # don't wait, just close queues
        exit_handler(1)
        #for async_worker in taskmgr.get_worker_pools():
        #    # finish processing jobs, close queues
        #    async_worker.finish()
        #    async_worker.close_queues()
        #sys.exit(1)

    # close fabric ssh connections
    fabricnetwork.disconnect_all()

    results_flat, results_ok, results_failed = print_statistics()
    if len(results_flat) and not len(results_failed):
        exit_code = 0
        l.info("")
        l.important("All was OK!")
    else:
        exit_code = 1
    sys.exit(exit_code)
    # print final statistics 
    #results = dict()
    #l.info("=" * 80)
    #l.info("Statistics: ")
    #for async_worker in taskmgr.get_worker_pools():
    #    while not async_worker.result_queue_empty():
    #        host, cluster, result = async_worker.result_queue_get(block=True)
    #        if not cluster in results:
    #            results[cluster] = dict()
    #        results[cluster][host] = result
    #print results
    #results_flat = [host for cluster in results.itervalues() for host in cluster.itervalues()]
    #results_ok = [i for i in results_flat if i == True]
    #results_failed = [i for i in results_flat if i == False]
#
    #l.info('Total: %d' % len(results_flat))
    #l.info('OK: %d/%0.2f%% |  Failed: %d/%0.2f%%' % (len(results_ok), len(results_ok)/len(results_flat)*100, len(results_failed), len(results_failed)/len(results_flat)*100))
    #if len(results_failed):
    #    l.info("")
    #    l.info("-" * 60)
    #    l.info("Failed hosts:")
    #    for cluster in sorted(results.keys()):
    #        for host in results[cluster].keys():
    #            if results[cluster][host] == False:
    #                l.info("%s / %s" % (cluster, host))
    #    sys.exit(1)
    #else:
    #    l.info("")
    #    l.info("All was OK!")
    #    sys.exit(0)

    # todo:
    # child to signal parent if exception to stop feeding new jobs
    # statistics not realistic when check name doesn't exist and check fails because of this
    # review exit_handler and print statistic


