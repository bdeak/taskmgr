#!/usr/bin/env python
# -*- coding: UTF-8 -*-

# needed to properly import tasks, and to access internal fabric functionality
from fabric import main as fabricmain 
from fabric import network as fabricnetwork
from fabric.api import *

import paramiko

import utils.log
import argparse
import logging
import configparser
import os.path
import sys
import os
import time
import subprocess
import re
import signal
import getpass

import multiprocessing
import Queue


__version__ = "0.7.0"

class OperationException(Exception):
    pass

class ParseException(Exception):
    pass

class TaskRunException(Exception):
    pass

class NoSuchTaskException(Exception):
    pass

class StopException(Exception):
    pass

class TaskTimeoutException(Exception):
    pass

class TaskMgr(object):

    def __init__(self):
        # read cli arguments
        self.args = self.parse_cli_args()
        self.config = dict()
        self.worker_pools = dict()
        self.ssh_config = None
        self.clusters = None

    def get_arguments(self):
        return self.args

    def read_configuration(self, configfile, ctype, allow_no_value=False):
        config = configparser.ConfigParser(allow_no_value=allow_no_value)
        config._interpolation = configparser.ExtendedInterpolation()
        try:
            config.read(configfile)
        except Exception as e:
            raise OperationException("Failed to read type '%s' config file %s: %s" % (ctype, configfile, str(e)))
        self.config[ctype] = config._sections

    def read_commands_file(self, configfile, ctype):
        """
        A simple replacement for configparser, because commands.ini has a different syntax, and ConfigParser
        in python 2.7 doesn't allow to change the delimiter when parsing a config file
        This will read every line in each section and return them within a dict, key names are the sections, values are lists of the lines
        This allows handling lines which are exactly the same (if one goes crazy with the configuration)
        """
        if not os.path.exists(configfile):
            handle_exception("Provided commands file '%s' doesn't exist!" % configfile)
        section = None
        config = dict()
        with open(configfile) as f:
            for line in f:
                line = line.rstrip()
                if re.search("^[#;]", line):
                    continue
                if re.search("^\s*$", line):
                    continue
                m = re.search("^\[([^\]]+)]\s*$", line)
                if m:
                    section = m.group(1)
                    continue
                else:
                    if section is None:
                        raise OperationException("Can't parse config file '%s': data without section has been found." % configfile)
                    else:
                        if not section in config.keys():
                            config[section] = list()
                        # store every line as keys, value is None, as we don't have key/value pairs
                        config[section].append(line)
        self.config[ctype] = config

    def get_configuration(self, ctype):
        return self.config[ctype]
    
    def parse_cli_args(self):
        # parse command line arguments
        parser = argparse.ArgumentParser(add_help=False)
        mandatory = parser.add_argument_group(title='Mandatory arguments')
        mandatory.add_argument("-n", "--cluster-name", help="The name of the cluster to work on. Can be provided multiple times.", action='append', required=True)
        mandatory.add_argument("-c", "--command", help="THe name of the command to run", required=True)
        optional = parser.add_argument_group(title='Optional arguments')
        optional.add_argument("--cluster-config", help="The location of the cluster configuration file", default="cluster.ini")
        optional.add_argument("--command-config", help="The location of the command configuration file", default="commands.ini")
        optional.add_argument("--config", help="The location of the taskmgr configuration file", default="config.ini")      
        optional.add_argument("-H", "--hosts-file", help="File storing the hosts to work on, one by line. If not used, the hostnames from cluster.ini are used.") # fixme
        optional.add_argument("-d", "--debug", help="Show debug information", action="store_true", default=False)
        optional.add_argument("-x", "--unsafe", help="Enable unsafe operation. (disable host key checking, etc)", action='store_true', default=False)
        optional.add_argument("--show-exception", help="Show exception when tasks fail", action='store_true', default=False)
        optional.add_argument("--ssh-config", help="The ssh configuration file to use.", default="%s/.ssh/config" % os.environ["HOME"])
        optional.add_argument("-f", "--force", help="Use force, like not asking confirmation for custom commands", action="store_true", default=False)
        optional.add_argument("--print-unprocessed", help="When using hostlist, print which hosts would not be processed due to lack of matching pattern", action="store_true", default=False)
        optional.add_argument("-v", "--version", action="version", version=__version__)
        optional.add_argument("-h", "--help", help="Show this help", action="help")

        args = parser.parse_args()
        return args

    def append_worker_pool(self, cluster, pool):
        self.worker_pools[cluster] = pool

    def get_worker_pools(self):
        return self.worker_pools

    def get_node_list_for_cluster(self, cluster):

        # check if the cluster definition exists
        if not cluster in self.config["cluster"].keys():
            handle_exception("Configuration in '%s' for cluster '%s' doesn't exist!" % (self.args.cluster_config, cluster))

        # check if a hostlist was provided
        if not self.args.hosts_file:
            # not provided, use the hostnames defined in cluster.ini
            hosts = subprocess.Popen(["/bin/bash", "-c", "echo %s" % self.config["cluster"][cluster]['hosts']], stdout=subprocess.PIPE).communicate()[0].rstrip().split(" ")
        else:
            # get the hosts from the host file which are matching the pattern given for the current cluster in cluster.ini
            # expand the expression found in cluster.ini using bash - might need to rewrite this in pure python?
            hosts = list()
            try:
                f = open(self.args.hosts_file)
                for host in f.readlines():
                    if re.search("(^\s*$|^#)", host):
                        continue
                    for pattern in self.config["cluster"][cluster]["pattern"].split(" "):
                        if re.search(pattern, host, flags=re.IGNORECASE):
                            hosts.append(host.rstrip())
            except Exception as e: 
                handle_exception("Problem while reading host file '%s': %s" % (self.args.hosts_file, str(e)))
        return hosts

    def get_unmatched_hosts(self):
        """ return all the nodes from hosts-file which doesn't match any patterns """

        # get the hosts from the host file which are matching the pattern given for the current cluster in cluster.ini
        # expand the expression found in cluster.ini using bash - might need to rewrite this in pure python?
        hosts = list()
        found = dict()
        try:
            f = open(self.args.hosts_file)
            for host in f.readlines():
                host = host.rstrip()
                if re.search("(^\s*$|^#)", host):
                    continue
                for cluster in self.config["cluster"].keys():
                    for pattern in self.config["cluster"][cluster]["pattern"].split(" "):
                        if re.search(pattern, host, flags=re.IGNORECASE):
                            found[host] = True
                
                if not host in found.keys():
                    hosts.append(host.rstrip())                            

        except Exception as e: 
            handle_exception("Problem while reading host file '%s': %s" % (self.args.hosts_file, str(e)))
        return sorted(set(hosts))

    def get_full_node_list(self):
        hosts = list()
        for cluster in sorted(self.clusters):
            hosts.append(self.get_node_list_for_cluster(cluster))
        # return a flattened list (instead of lists of lists)
        return sorted([item for sublist in hosts for item in sublist])


    def read_ssh_config(self):
        self.ssh_config = paramiko.config.SSHConfig()
        self.ssh_config.parse(file(self.args.ssh_config))

    def lookup_ssh_config_for_host(self, hostname):
        if self.ssh_config is None:
            self.read_ssh_config()
        return self.ssh_config.lookup(hostname)

    def get_ssh_users(self):
        """ 
        Go through the ssh cofig file and detect what users are used for connacting hosts
        This will be needed to collect all the necessary passwords in advance
        """
        usernames = list()
        with open(self.args.ssh_config) as f:
            for line in f:
            	if re.search("^\s*#", line):
            		continue
                m = re.search("User ([^ ]+)", line, re.IGNORECASE)
                if m:
                    usernames.append(m.group(1).rstrip())
        # while returning get rid of duplicates
        return sorted(set(usernames))

    def set_clusters(self, clusters):
        self.clusters = clusters

def handle_exception(exception, *args, **kwargs):
    global pid
    if "fatal" in kwargs.keys():
        fatal = kwargs["fatal"]
    else:
        fatal = True
    
    l.critical(exception, *args)
    
    if os.getpid() == pid:
        # this is the parent process, do cleanup
        if fatal:
            exit_handler(1)
    else:
        # child process
        exit_if_fatal(fatal)

def exit_if_fatal(fatal=True):
    if fatal:
        # close fabric ssh connections
        fabricnetwork.disconnect_all()
        sys.exit(1)



def exit_handler(exit_code=None):
    global taskmgr
    for cluster, async_worker in taskmgr.get_worker_pools().items():
        # finish processing jobs, close queues
        async_worker.finish()
        async_worker.close_queues()

    # close fabric ssh connections
    fabricnetwork.disconnect_all()

    if exit_code is None or exit_code == 0:
        results_flat, results_ok, results_failed = print_statistics()
        
        if exit_code is None:
            if len(results_failed):
                exit_code = 1
            else:
                exit_code = 0
                l.info("")
                l.info("All was OK!")
    sys.exit(exit_code)

def print_statistics():
    global taskmgr
    results = dict()
    actual_list = list()

    for cluster, async_worker in taskmgr.get_worker_pools().items():
        while not async_worker.result_queue_empty():
            host, cluster, result = async_worker.result_queue_get(block=True)
            if not cluster in results:
                results[cluster] = dict()
            results[cluster][host] = result
            actual_list.append(host)

    results_flat = [host for cluster in results.itervalues() for host in cluster.itervalues()]
    results_ok = [i for i in results_flat if i == True]
    results_failed = [i for i in results_flat if i == False]

    # fixme: print the list of nodes that were not executed, using a diff of the two lists
    expected_list = taskmgr.get_full_node_list()
    # get the difference of the expected and the actual lists
    # danger: this will kill ordering (ok), and remove unique elements (should be ok, one host should only be part of one cluster)
    skipped_list = sorted(list(set(expected_list) - set(actual_list)))

    if len(results_flat) > 0:
        l.info("=" * 80)
        l.info("Statistics: ")
        l.info("")
        l.info("Expected: %d" % len(expected_list))
        l.info('Total: %d' % len(results_flat))
        l.info('OK: %d/%0.2f%% |  Failed: %d/%0.2f%%' % (len(results_ok), float(len(results_ok))/float(len(results_flat))*100, len(results_failed), float(len(results_failed))/float(len(results_flat))*100))
        if len(results_failed):
            l.info("")
            l.info("-" * 60)
            l.info("Failed hosts:")
            for cluster in sorted(results.keys()):
                for host in sorted(results[cluster].keys()):
                    if results[cluster][host] == False:
                        l.info("%s / %s" % (cluster, host))
        if len(skipped_list):
            l.info("")
            l.info("-" * 60)
            l.info("Skipped hosts:")
            for host in skipped_list:
                l.info(host)

    return (results_flat, results_ok, results_failed, skipped_list)

def worker(host, cluster, config, args):
    """ The worker function. This is the code that gets executed for each cluster/node combination.
        The final result will represent the success or the failure of the given command on the given machine
        In case of a problem an exception is raised.
    """
    global timeout
    global command_timeout

    # set timeout values for facter
    env.timeout = timeout
    env.command_timeout = command_timeout
    env.abort_on_prompts = True
    env.use_ssh_config = True
    if args.unsafe:
        env.disable_known_hosts = True
        env.reject_unknown_hosts = False
        env.skip_bad_hosts = False
    else:
        env.disable_known_hosts = False
        env.reject_unknown_hosts = True
        env.skip_bad_hosts = True

    # set the password for the given user in env.passwords
    ssh_config = taskmgr.lookup_ssh_config_for_host(host)
    try:
        ssh_port = ssh_config["port"]
    except:
        ssh_port = 22
    try:
        ssh_user = ssh_config["user"]
    except:
        ssh_user = getpass.getuser()

    env.passwords['%s@%s:%s' % (ssh_user, host, ssh_port)] = passwords[ssh_user]

    # ignore ctrl c here
    signal.signal(signal.SIGINT, signal.SIG_IGN)

    # command1: segment1 segment2 segment3
    # command2: segment1 segment2 segment3
    for command in config["command"][args.command]:

        # set defaults for all flags
        flags = dict()
        flags["skip"] = False
        flags["stop"] = False
        flags["negate"] = False
        flags["multiple"] = True
        flags["tries"] = 1
        flags["delay"] = 120

        # there are two types of input in commands.ini:
        # 1) check|wait|execute [not] command[:arguments] [skip|stop|tries=1|delay=120] (default: stop)
        # 2) check|wait|execute [not] cmd:task.name:arguments [cmd:task.name2:arguments] [skip|stop|all|any|tries=1|delay=120]

        # parse these two differently
        # check if 2) matches (:cmd)
        if not command.find(" cmd:") == -1:
            # type 2)
            cmd_name = "direct"

            # do a search and replace for placeholder '%HOST%'
            command.replace("%HOST%", host)

            m = re.search("^([^ ]+)(?: (not))?( cmd:[^\]]+)(?: \[([^\]]+)])?\s*$", command)
            if m:
                action, negate, tmp_command, tmp_flags = m.groups()
                if not tmp_flags is None:
                    for flag in tmp_flags.split(","):
                        if len(flag.split("=")) > 1:
                            # key-value pair
                            key, value = flag.split("=")
                            flags[key] = value
                        else:
                            # booleans
                            flags[flag] = True

                # sanity checking, set defaults
                try:
                    if flags["skip"] and flags["stop"]:
                        handle_exception("Providing both 'skip' and 'stop' flags doesn't make any sense! (%s: %s)" % (args.command, command), host, cluster)
                except KeyError:
                    pass

                # sanity checking, set defaults
                if "any" in flags.keys():
                    flags["multiple"] = False

                try:
                    if flags["all"] and flags["any"]:
                        handle_exception("Providing both 'all' and 'any' flags doesn't make any sense! (%s: %s)" % (args.command, command), host, cluster)
                except KeyError:
                    pass

                if negate:
                    flags["negate"] = True

            else:
                handle_exception("Can't parse command '%s/%s'" % (args.command, command), host, cluster)

            cmd_segments = filter(bool, re.split(" ?cmd:", tmp_command, flags=re.IGNORECASE))

        else:
            # type 1)            
            m = re.search("^(check|wait|execute)(?: (not))? ([^ :]+)(?::(.+?))?(?: \[(skip|stop|tries=1|delay=120)\])?$", command)
            if m:
                action, negate, cmd_name, cmd_arguments, tmp_flags = m.groups()
            else:
                raise AttributeError("Can't parse the command definition for command '%s' in '%s'" % (command, args.command_config))

            if not tmp_flags is None:
                for flag in tmp_flags.split(","):
                    if len(flag.split("=")) > 1:
                        # key-value pair
                        key, value = flag.split("=")
                        flags[key] = value
                    else:
                        # booleans
                        flags[flag] = True

            # sanity checking, set defaults
            try:
                if flags["skip"] and flags["stop"]:
                    handle_exception("Providing both 'skip' and 'stop' flags doesn't make any sense! (%s: %s)" % (args.command, command), host, cluster)
            except KeyError:
                pass

            if negate:
                flags["negate"] = True
       
            # check if a counterpart in cluster.ini exists
            # pair the check name with the one that is defined in the cluster configuration
            if not cmd_name in config['cluster'][cluster]:
                if config["taskmgr"]["general"]["unknown_command_behavior"] == "fail":
                    handle_exception("Referenced command '%s' is not defined in in '%s' for cluster '%s'!" % (cmd_name, args.cluster_config, cluster), host, cluster)
                elif config["taskmgr"]["general"]["unknown_command_behavior"] == "ok":
                    tmp_command = "cmd:checks.dummy:ok"
                else:
                    handle_exception("Provided value '%s' for 'unknown_command_behavior' is unknown in taskmgr configuration '%s'" % (config["taskmgr"]["general"]["unknown_command_behavior"], args.config), host, cluster)
            else:
                # run the check, fail if result is false
                # remove all|any form the end
        
                # split the command on 'cmd:'
                # check if ' (all|any)$' matches the command
                tmp_command = config["cluster"][cluster][cmd_name].replace("%HOST%", host)

            # if arguments are passed from commands.ini, add them to tmp_command 
            if not cmd_arguments is None:
                tmp_command = tmp_command.replace('%ARGS%', cmd_arguments)

            # sanity check
            try:
                i = tmp_command.index('%ARGS%')
                raise ValueError("Placeholder '%ARGS%' is used for command '%s', but no arguments are passed from '%s'!" % (cmd_name, args.command_config), host, cluster)
            except:
                pass
    
            m = re.search("^(.*) \[(.*)\]$", tmp_command, flags=re.IGNORECASE)
            if m:
                # found, set multiple, and remove it from the end of the string
                tmp_command, tmp_flags = m.groups()
                if not tmp_flags is None:
                    for flag in tmp_flags.split(","):
                        if len(flag.split("=")) > 1:
                            # key-value pair
                            key, value = flag.split("=")
                            flags[key] = value
                        else:
                            # booleans
                            flags[flag] = True

                # sanity checking, set defaults
                if "any" in flags.keys():
                    flags["multiple"] = False

                try:
                    if flags["all"] and flags["any"]:
                        handle_exception("Providing both 'all' and 'any' flags doesn't make any sense! (%s: %s)" % (args.command, command), host, cluster)
                except KeyError:
                    pass

            else:
                # default is 'all'
                pass
    
            # split commands on 'cmd:', and filter out any empty elements
            cmd_segments = filter(bool, re.split(" ?cmd:", tmp_command, flags=re.IGNORECASE))

        # call the commands
        # rule of thumb: on any failed check we return with false, meaning that the command failed
        # iterate on the segments, process them one by one
        if action == "check":
            try:
                result = run_check_tasks(host, cluster, cmd_name, cmd_segments, flags)
                if flags["negate"]:
                    result = not result
            except Exception as e:
                raise
            if not result:
                l.error("%s '%s' failed" % (action, cmd_name), host, cluster)
                if flags["skip"]:
                    l.warning("Error action for %s '%s' is 'skip', moving on with next host" % (action, cmd_name), host, cluster)
                    return (host, cluster, False)
                elif flags["stop"]:
                    raise StopException("Requested action for %s %s on %s/%s is '%s'" %(action, cmd_name, cluster, host, flags))
            else:
                l.info("%s '%s' OK" % (action, cmd_name), host, cluster)
        
        elif action == "wait":
            # stop and wait until check returns true
            try:
                result = run_wait_tasks(host, cluster, cmd_name, cmd_segments, flags)
            except Exception as e:
                raise
            if not result:
                # for this we would need to implement timeouts in run_wait_tasks, because right now False is never returned
                # in this case will need to evaluate 'erroraction' 
                l.error("%s '%s' failed" % (action, cmd_name), host, cluster)
                return (host, cluster, False)
            else:
                l.info("%s '%s' OK" % (action, cmd_name), host, cluster)
        
        elif action == "execute":
            # do multiple iterations if needed - sometimes execute calls may fail, like dpkg is locked, etc
            for counter in range(int(flags["tries"])):
                try:
                    result = run_execute_tasks(host, cluster, cmd_name, cmd_segments, flags)
                    if flags["negate"]:
                        result = not result
                except Exception as e:
                    # don't try again on an exception - might need to reconsider later
                    raise
                if not result:
                    if counter == int(flags["tries"]) - 1:
                        # last try
                        l.error("%s '%s' failed" % (action, cmd_name), host, cluster)
                        if flags["skip"]:
                            l.warning("Error action for %s '%s' is 'skip', moving on with next host, but not stopping" % (action, cmd_name), host, cluster)
                            return (host, cluster, False)
                        elif flags["stop"]:
                            raise StopException("Requested action for %s %s on %s/%s is '%s'" %(action, cmd_name, cluster, host, flags))
                    else:
                        # retry
                        l.warning("%s '%s' failed, but retrying %d more time(s) after %d seconds of delay" % (action, cmd_name, int(flags["tries"])-1-counter, int(flags["delay"])), host, cluster)
                        time.sleep(int(flags["delay"]))
                        continue
                else:
                    l.info("%s '%s' OK" % (action, cmd_name), host, cluster)
                    break
        else:
            handle_exception("Unknown command '%s' defined in '%s'!" % (cmd_name, args.command_config), host, cluster)    

    l.important("All commands for %s/%s have been successfully run" % (cluster, host), host, cluster)
    return (host, cluster, True)

def run_check_tasks(host, cluster, cmd_name, cmd_segments, flags):
    """ 
        A helper task called from worker() to execute a check task

        host: host on which the given task is running
        cluster: cluster of which the given host is part of
        cmd_name: name of the given command that is running (host_alive, etc)
        cmd_segments: list containing the command segments that will be run
        flags: Dictionary holding the flags that modify the behavior of task runs
            flags["multiple"] = true/false, showing how many of the cmd_segments need to return success in order to consider the
                  result to be success. True: all of them, False, at least one.
    """
    global timeout
    global command_timeout
    global callables
    results = list()
    for cmd in cmd_segments:
        check_name = cmd.split(":")[0].rstrip()
        check_arguments = ":".join(cmd.split(":")[1:]).rstrip()
        if not check_name in fabricmain._task_names(callables):
            raise NoSuchTaskException("Fabric task '%s' doesn't exist!" % check_name)

        try: 
            l.info("Executing check %s: '%s %s'" % (cmd_name, check_name, check_arguments), host, cluster)
            with quiet():
                result = execute(check_name, check_arguments, cluster, hosts=[host])
        except Exception as e:
            raise TaskRunException("Failed to run fabric task '%s' on host '%s' (cluster '%s'): %s" % (check_name, host, cluster, str(e)))

        # got the result, good
        if flags["multiple"]:
            if result[host]:
                results.append(result[host])
                continue
            else:
                return False
        else:
            if result[host]:
                return True
            else:
                continue
                results.append(result[host])
    # if we are here all of the checks have been run
    # either because multiple is True, or because multiple is False, but
    # the first checks failed
    if flags["multiple"]:
        return all(res == True for res in results)
    else:
        return any(res == True for res in results)

def run_wait_tasks(host, cluster, cmd_name, cmd_segments, flags):
    """ 
    A helper task called from worker() to execute a wait task
    """
    global timeout
    global command_timeout
    global callables

    results = dict()

    segment_index = 0
    counter = 1
    while True:
        cmd = cmd_segments[segment_index]
        check_name = cmd.split(":")[0].rstrip()
        check_arguments = ":".join(cmd.split(":")[1:]).rstrip()
        # check if a given fabric task exists
        if not check_name in fabricmain._task_names(callables):
            raise NoSuchTaskException("Fabric task '%s' doesn't exist!" % check_name)

        l.debug("Starting wait %s: '%s %s'" % (cmd_name, check_name, check_arguments), host, cluster)

        # if there is only one check, don't use the timeout for iteration, but wait until it get's OK
        if len(cmd_segments) > 1:
            wait_timeout = 60
        else:
            wait_timeout = None

        try:
            result = run_wait_task_helper(host, cluster, check_name, check_arguments, flags, wait_timeout)
        except TaskTimeoutException as e:
            # task timed out without getting an OK result, move on
            results[segment_index] = False
            l.info("Timeout reached while waiting for '%s': '%s %s', moving on with next check" % (cmd_name, check_name, check_arguments), host, cluster)
        except TaskRunException:
            # task run has failed
            raise
        except Exception as e:
            # some other exception
            raise

        else:
            # no timeout has been reached, result must be true
            if flags["multiple"]:
                # true result, but we need all the checks to be true, so we store the result and move on
                results[segment_index] = True
            else:
                # true result, and we only need one true result, return with true
                return True

        # check if we are at the end of the results list, if yes, evaluate it's contents
        if segment_index == len(cmd_segments) - 1:
            if flags["multiple"]:
                # evaluate the contents of results
                if all(res == True for res in results.values()):
                    return True
            else:
                # we shouldn't be here ever
                pass
        
        # move on to next element
        # treat the results list as a ring buffer by manipulating the index we use to reference values within the list
        segment_index = counter % len(cmd_segments)
        counter += 1


def run_wait_task_helper (host, cluster, check_name, check_arguments, flags, wait_timeout=60):
    """ 
    Helper for run_wait_task(), will run one wait check until timeout is reached
    Needed to be able to easily get out of two 'while True' loops
    """
    # get the start time, will be used to measure if 'wait_timeout' has been reached
    start_time = int(time.time())
    while True:

        # this loop is needed to ensure we wait until the check is finally ok, or wait_timeout is reached
        try:
            l.info("Executing wait '%s %s'" % (check_name, check_arguments), host, cluster)
            with quiet():             
                result = execute(check_name, check_arguments, cluster, hosts=[host])
                if flags["negate"]:
                    result[host] = not result[host]
        except Exception as e:
            raise TaskRunException("Failed to run fabric task '%s' on host '%s' (cluster '%s'): %s" % (check_name, host, cluster, str(e)))
        if result[host] == True:
            # break out of while True
            break
        else:
            # failed, have to wait longer
            l.debug("Waiting before retrying", host, cluster)
            time.sleep(10)

        # check if timeout has been reached
        if not wait_timeout is None:
            if int(time.time()) > start_time + wait_timeout:
                raise TaskTimeoutException("Wait check '%s' didn't return with OK state before wait_timeout has been reached." % (check_name))

    l.debug("Got response, moving on", host, cluster)
    return True      

def run_execute_tasks(host, cluster, cmd_name, cmd_segments, flags):
    """ 
        A helper task called from worker() to execute an 'execute' task
    """
    global timeout
    global command_timeout
    global callables
    results = list()
    for cmd in cmd_segments:
        check_name = cmd.split(":")[0].rstrip()
        check_arguments = ":".join(cmd.split(":")[1:]).rstrip()
        if not check_name in fabricmain._task_names(callables):
            raise NoSuchTaskException("Fabric task '%s' doesn't exist!" % check_name)

        try: 
            l.info("Executing %s: '%s %s'" % (cmd_name, check_name, check_arguments), host, cluster)
            with quiet():
                result = execute(check_name, check_arguments, cluster, hosts=[host])
        except Exception as e:
            raise TaskRunException("Failed to run fabric task '%s' on host '%s' (cluster '%s'): %s" % (check_name, host, cluster, str(e)))

        # got the result, good
        if flags["multiple"]:
            if result[host]:
                results.append(result[host])
                continue
            else:
                return False
        else:
            if result[host]:
                return True
            else:
                continue
                results.append(result[host])
    # if we are here all of the checks have been run
    # either because multiple is True, or because multiple is False, but
    # the first checks failed
    if flags["multiple"]:
        return all(res == True for res in results)
    else:
        return any(res == True for res in results)


# due to a possible bug in the signal handling of multiprocessing's Pool implementation
# we need our own pool management
# http://bryceboe.com/2010/08/26/python-multiprocessing-and-keyboardinterrupt/
class AsyncWorker:
    def __init__(self, cluster, show_exception=False):
        self.job_queue = multiprocessing.Queue()
        self.result_queue = multiprocessing.Queue() 
        self.workers = list()
        self.failed_clusters = dict()
        self.show_exception = show_exception
        self.cluster = cluster

    def add_job(self, host, cluster, config, args):
        self.job_queue.put((host, cluster, config, args))

    def start_workers(self, worker_func, max_parallel):

        # if number of elements in the job queue (for this cluster) are smaller than the parallel level,
        # don't start as much workers
        # the qsize() method doesn't return a reliable number, but at this point no other processes should be
        # putting anything to the job queue for this cluster worker
        if self.job_queue.qsize() > max_parallel:
            iterrate = max_parallel
        else:
            iterrate = self.job_queue.qsize()

        for i in range(iterrate):
            w = multiprocessing.Process(target=self._worker_helper, args=(worker_func,))
            w.daemon = True
            w.start()
            self.workers.append(w)

    def _worker_helper(self, worker_func):
        while True:
            try:
                # get a new element from the queue
                # blocking mode, and give one second for the element to be ready
                host, cluster, config, args = self.job_queue.get(block=True, timeout=1)

                # implement failing policy:
                # we don't allow any more jobs for faulty nodes
                # but we allow other nodes to finish
                # we don't start any new nodes
                # flags are set in the exception handling part
                if cluster in self.failed_clusters.keys():
                    if self.failed_clusters[cluster]['!alerted!'] == False:
                        l.error("Due to previous errors, new nodes on cluster '%s' are not processed. Already running tasks are allowed to finish." % cluster, host, cluster)
                        self.failed_clusters[cluster]['!alerted!'] = True
                    if host in self.failed_clusters[cluster].keys():
                        if self.failed_clusters[cluster][host] == False:
                            # first occurance, notify
                            l.error("Due to previous errors, no more tasks on '%s/%s' are executed as of now." % (cluster, host), host, cluster)
                            self.failed_clusters[cluster][host] = True
                            continue
                        else:
                            l.warning("skipping instead of starting")
                            continue
                    else:
                        # in the marked cluster, but not marked host, don't allow to continue
                        l.debug("skipping, because cluster is marked as faulty", host, cluster)
                        continue
                else:
                    # not marked cluster, allow to continue
                    pass

                # if here, start the task
                l.debug("Starting worker", host, cluster)
                self.result_queue.put(worker_func(host, cluster, config, args)) # put the results to the queue

            except Queue.Empty:
                l.debug("Queue is empty, exiting from _worker_helper()", "-", self.cluster)
                break
            except KeyboardInterrupt:
                pass
            except Exception as e:

            	if self.show_exception:
                    l.exception(e, host, cluster)
                else:
					l.critical("Running task failed with exception: %s" % e, host, cluster)

                # this indicates that a vital task has failed (erroraction==stop)
                # or in case there was only one check at a command, then that one check failed
                # in this case put a result entry in the queue
                self.result_queue.put((host, cluster, False))
                
                # add the given cluster/node to the faulty clusters list
                # mark the cluster as faulty by creating the cluster key if it doesn't already exist
                if not cluster in self.failed_clusters.keys():
                    self.failed_clusters[cluster] = dict()
                    self.failed_clusters[cluster]["!alerted!"] = False

                # mark the node as faulty by adding it to the dict
                # False means that it has not been notified yet
                # after notification flag will be turned true to avoid re-notification
                if not host in self.failed_clusters[cluster].keys():
                    self.failed_clusters[cluster][host] = False

    def wait(self):
        for worker in self.workers:
            worker.join()

    def finish(self):
        for worker in self.workers:
            worker.terminate()
            worker.join()

    def result_queue_empty(self):
        return self.result_queue.empty()

    def result_queue_get(self, block=False):
        return self.result_queue.get(block)

    def close_queues(self):
        while not self.job_queue.empty():
            self.job_queue.get()
        self.job_queue.close()


###################################################################################3

# global variable holding logger object
#l = object
taskmgr = object
# timeouts to use
timeout = 10
command_timeout = None
pid = os.getpid()
passwords = dict()

# load available fabric task files 
# http://stackoverflow.com/questions/23605418/in-fabric-how-can-i-execute-tasks-from-another-python-file
docstring, callables, default = fabricmain.load_fabfile('fabfile')
fabricmain.state.commands.update(callables)

# initialize logger
try:
    format_file = '%(asctime)s %(clusterinfo)s - %(levelname)s - %(message)s'
    format_console = '$COLOR%(clusterinfo)s%(levelname)s - %(message)s'
    l = utils.log.initialize_logger('taskmgr.log', format_file, format_console)
except Exception as e:
    print "Can't initialize logger: ", str(e)
    sys.exit(1)

if __name__ == '__main__':

    config = dict()

    # create taskmanager object
    taskmgr = TaskMgr()
    args = taskmgr.get_arguments()

    # set the loglevel based on args.debug
    if args.debug:
        loglevel = logging.DEBUG
    else:
        loglevel = logging.INFO
    #l.setLevel(loglevel)
    logging.getLogger().setLevel(loglevel)
    
    # make paramiko shut up
    logging.getLogger("paramiko").setLevel(logging.CRITICAL)

    # parse the cluster configuration
    try:
        taskmgr.read_configuration(args.cluster_config, ctype="cluster")
    except OperationException as e:
        handle_exception(e)
    # get the parsed configuration in the form of a dict
    config["cluster"] = taskmgr.get_configuration("cluster")

    # parse the command configuration
    try:
        taskmgr.read_commands_file(args.command_config, ctype="command")
    except OperationException as e:
        handle_exception(e)
    # get the parsed configuration in the form of a dict
    config["command"] = taskmgr.get_configuration("command")

    # parse the default configuration
    try:
        taskmgr.read_configuration(args.config, ctype="taskmgr")
    except OperationException as e:
        handle_exception(e)
    # get the parsed configuration in the form of a dict
    config["taskmgr"] = taskmgr.get_configuration("taskmgr")    

    if not args.command in config["command"].keys():
        handle_exception("Configuration in '%s' for command '%s' doesn't exist!" % (args.command_config, args.command))

    # look at the parsed commands file for the requested command, and check for custom_cmd entries.
    # if found, print them and ask for user confirmation (unless --force is used) before continuing
    if not args.force:
        custom_cmds = list()
        for cmd in config["command"][args.command]:
            if re.search("custom_cmd", cmd):
                m = re.search("^([^ ]+)(?: (not))?( cmd:[^\]]+)(?: \[([^\]]+)])?\s*$", cmd)
                if m:
                    cmd_segments = filter(bool, re.split(" cmd:[^:]+:", m.group(3), flags=re.IGNORECASE))
                    for segment in cmd_segments:
                        with quiet():
                            l.debug("calling with: %s" % segment)
                            custom_cmds.append(execute("execute.custom_cmd.parse", segment, host="localhost")["localhost"][1])
                else:
                    # ask question, can't parse, might mean problems
                    l.warning("Custom_cmd found, but failed to parse line.")
                    custom.cmds.append(None)
        if len(custom_cmds):
            # there was at least custom command, or unparseable line

            # get rid of unique elements
            custom_cmds = set(custom_cmds)
            l.warning("Warning: The following custom commands have been found that will be executed: ")
            print ""
            for cmd in custom_cmds:
                print cmd
            print ""
            while True:
                answer = raw_input("Are you sure you want to continue? [y/N]").rstrip("\n")
                if not answer:
                    l.info("User abort, exiting.")
                    print ""
                    sys.exit(1)
                if answer == "y":
                    l.info("User answered yes, going on")
                    print ""
                    break

    if args.print_unprocessed:
        if not args.hosts_file:
            handle_exception("--print-unprocessed can only be used if --hosts_file is also used.")
        unprocessed_hosts = taskmgr.get_unmatched_hosts()
        print "Hosts that are in '%s' but due to no matching pattern will not be processed: " % args.hosts_file
        for host in unprocessed_hosts:
            print host
        sys.exit(0)

    if not os.path.exists(args.ssh_config):
        handle_exception("Provided ssh config file '%s' doesn't exist!" % args.ssh_config)

    # read the passwords for all users defined in ssh_config
    # fabric would do this, but because of parallel execution it doesn't always work
    users = taskmgr.get_ssh_users()
    if not len(users):
        # no users specified, use the current user
        users.append(getpass.getuser())
    else:
       # read in the password for each user
       for user in users:
            passwords[user] = getpass.getpass("Please provide password for user '%s': " % user)

    # if cluster name is 'all', then get all clusters from the cluster configuration sections and put it into args.cluster_name
    clusters = list()
    if len(args.cluster_name) == 1 and args.cluster_name[0] == "all":
        clusters = sorted(config["cluster"].keys())
    else:
        clusters = args.cluster_name

    # store the clusters in taskmgr
    taskmgr.set_clusters(clusters)

    # count how many machines will be processed - for statistics later on
    nodelist = taskmgr.get_full_node_list()

    # iterate on the provided cluster names
    nodelist = dict()
    for cluster in clusters:
        # sanity check - make sure one machine is only part of one cluster
        hosts = taskmgr.get_node_list_for_cluster(cluster)
        for host in hosts:
            if host not in nodelist.keys():
                nodelist[host] = cluster
            else:
                handle_exception("Host '%s' is member of multiple clusters: '%s' <-> '%s', exiting!" % (host, nodelist[host], cluster))

    for cluster in clusters:
        hosts = taskmgr.get_node_list_for_cluster(cluster)

        async_worker = AsyncWorker(cluster, args.show_exception)
        taskmgr.append_worker_pool(cluster, async_worker)

        # launch workers
        for host in hosts:
            l.debug("Adding job", host, cluster)
            async_worker.add_job(host, cluster, config, args)

        # launch the workers
        async_worker.start_workers(worker, int(config["cluster"][cluster]["max_parallel"]))

    try:
        for cluster, async_worker in taskmgr.get_worker_pools().items():
            l.debug("Waiting for cluster '%s' to finish" % cluster)
            async_worker.wait()
            l.debug("Cluster '%s' finished" % cluster)
            # close all queues
            async_worker.close_queues()
    except KeyboardInterrupt:
        print "Received Ctrl + C, exiting..."
        # don't wait, just close queues
        exit_handler(1)

    # close fabric ssh connections
    fabricnetwork.disconnect_all()

    results_flat, results_ok, results_failed, skipped_list = print_statistics()
    if len(results_flat) and not len(results_failed):
        if not len(skipped_list):
            exit_code = 0
            l.info("")
            l.important("All was OK!")
        else:
            # there were skipped hosts
            exit_code = 2
            l.info("")
            l.info("There were no errors, but there were some skipped hosts!") 
    else:
        exit_code = 1
    sys.exit(exit_code)



